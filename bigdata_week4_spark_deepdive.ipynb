{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/witsarut-big-data/128-356-Big-Data-for-test/blob/main/bigdata_week4_spark_deepdive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {
        "id": "cell_0"
      },
      "source": [
        "# üìò Big Data ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 4: Distributed Processing & Apache Spark (Deep Dive)\n",
        "\n",
        "\n",
        "**‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà:** 4  \n",
        "\n",
        "> üéØ **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ:** ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î Distributed Computing, ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡∏≠‡∏á Apache Spark, Lazy Evaluation, DAG ‡πÅ‡∏•‡∏∞ Shuffle ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏à‡∏£‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ PySpark\n",
        "\n",
        "**‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á**: Google Colab (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥) / Local Jupyter  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_1",
      "metadata": {
        "id": "cell_1"
      },
      "source": [
        "# 1. üñ•Ô∏è ‡∏ó‡∏≥‡πÑ‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏û‡∏≠? (Why Single Machine Fails)\n",
        "\n",
        "## ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "\n",
        "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏´‡∏•‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô:\n",
        "\n",
        "| ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ |\n",
        "|----------|----------|---------------|\n",
        "| **RAM** | ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏°‡∏µ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡πÄ‡∏ä‡πà‡∏ô 16 GB) | ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î CSV 50 GB ‡∏î‡πâ‡∏ß‡∏¢ Pandas ‚Üí MemoryError |\n",
        "| **CPU** | ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Core ‡∏°‡∏µ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡πÄ‡∏ä‡πà‡∏ô 4-8 Cores) | ‡∏á‡∏≤‡∏ô groupBy ‡∏ö‡∏ô 1 ‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô‡πÅ‡∏ñ‡∏ß ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á |\n",
        "| **I/O** | Disk ‡∏≠‡πà‡∏≤‡∏ô/‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ä‡πâ‡∏≤ | ‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏ü‡∏•‡πå 100 GB ‡∏à‡∏≤‡∏Å HDD ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ ~15 ‡∏ô‡∏≤‡∏ó‡∏µ |\n",
        "| **‡πÑ‡∏°‡πà‡∏Ç‡∏¢‡∏≤‡∏¢‡πÑ‡∏î‡πâ** | ‡πÄ‡∏û‡∏¥‡πà‡∏° RAM/CPU ‡∏°‡∏µ‡πÄ‡∏û‡∏î‡∏≤‡∏ô | ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏û‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ï‡∏•‡∏≤‡∏î‡∏Å‡πá‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î |\n",
        "\n",
        "### üí° ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Key Insight)\n",
        "\n",
        "> ‡πÄ‡∏°‡∏∑‡πà‡∏≠ **‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• √ó ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô > ‡∏Ç‡∏µ‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß**  \n",
        "> ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ **Distributed Computing** (‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢)\n",
        "\n",
        "### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ\n",
        "\n",
        "- **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß** = ‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏¢‡∏Å‡∏≠‡∏¥‡∏ê 10,000 ‡∏Å‡πâ‡∏≠‡∏ô ‚Üí ‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢ ‡∏ä‡πâ‡∏≤ ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤\n",
        "- **Distributed** = ‡∏Ñ‡∏ô 100 ‡∏Ñ‡∏ô ‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏¢‡∏Å‡∏Ñ‡∏ô‡∏•‡∏∞ 100 ‡∏Å‡πâ‡∏≠‡∏ô ‚Üí ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡πá‡∏ß!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "img_single_vs_distributed",
      "metadata": {
        "id": "img_single_vs_distributed"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/single_vs_distributed.png?raw=1\" width=\"800\" alt=\"Single Machine vs Distributed Computing: ‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ö‡∏Å‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏Å vs ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÅ‡∏ö‡∏Å\">\n",
        "  <br><i>Single Machine vs Distributed Computing: ‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ö‡∏Å‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏Å vs ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÅ‡∏ö‡∏Å</i>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_2",
      "metadata": {
        "id": "cell_2"
      },
      "outputs": [],
      "source": [
        "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "import psutil, os\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "cpu_count = os.cpu_count()\n",
        "\n",
        "print(f\"üñ•Ô∏è ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏µ RAM: {ram_gb:.1f} GB\")\n",
        "print(f\"üîß ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô CPU Cores: {cpu_count}\")\n",
        "print(f\"\\nüí° ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ {ram_gb:.0f} GB ‚Üí Pandas ‡∏à‡∏∞ crash!\")\n",
        "print(f\"üí° ‡∏ñ‡πâ‡∏≤ query ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà {cpu_count} cores ‚Üí ‡∏ä‡πâ‡∏≤!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_3",
      "metadata": {
        "id": "cell_3"
      },
      "source": [
        "### ‚úèÔ∏è ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 1: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Log 500 GB/‡∏ß‡∏±‡∏ô ‡∏ï‡πâ‡∏≠‡∏á JOIN ‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á 50 GB ‡πÅ‡∏•‡∏∞ GROUP BY  \n",
        "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á Server ‡∏°‡∏µ RAM 64 GB, 16 Cores\n",
        "\n",
        "**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°** (‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_4",
      "metadata": {
        "id": "cell_4"
      },
      "outputs": [],
      "source": [
        "# ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
        "\n",
        "# 1) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏µ‡πà GB?\n",
        "total_data_gb = ________  # ‡πÄ‡∏ï‡∏¥‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "\n",
        "# 2) RAM ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (True/False)\n",
        "ram_enough = ________  # True ‡∏´‡∏£‡∏∑‡∏≠ False\n",
        "\n",
        "# 3) ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô disk ‡πÑ‡∏î‡πâ 200 MB/s ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏µ‡πà‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì)?\n",
        "read_time_sec = ________  # ‡πÄ‡∏ï‡∏¥‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "\n",
        "# 4) ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Distributed Computing ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "answer = \"________\"\n",
        "\n",
        "print(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°: {total_data_gb} GB\")\n",
        "print(f\"RAM ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠: {ram_enough}\")\n",
        "print(f\"‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô disk: {read_time_sec} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚âà {read_time_sec/60:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "print(f\"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_5",
      "metadata": {
        "id": "cell_5"
      },
      "source": [
        "# 2. üî• Apache Spark ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "## ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°\n",
        "\n",
        "Apache Spark ‡∏Ñ‡∏∑‡∏≠ **Distributed Computing Engine** (‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏ô‡∏ï‡πå‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà\n",
        "\n",
        "> Spark ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô \"‡∏™‡∏°‡∏≠‡∏á‡∏Å‡∏•‡∏≤‡∏á\" ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏£‡πâ‡∏≠‡∏¢‡∏ï‡∏±‡∏ß‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô\n",
        "\n",
        "## ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏Å\n",
        "\n",
        "| ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ | ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö |\n",
        "|-----------|----------|-------------|\n",
        "| **In-Memory Processing** | ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ô RAM ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô disk ‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô | ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Hadoop MapReduce 10-100 ‡πÄ‡∏ó‡πà‡∏≤ |\n",
        "| **DAG Execution** | ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô Graph ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á | ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡πâ‡∏≤‡∏ô |\n",
        "| **Fault Tolerance** | ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏û‡∏±‡∏á ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ | ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏µ‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏™‡∏π‡∏ï‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£ |\n",
        "| **Unified Engine** | ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö SQL, ML, Streaming, Graph | ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á |\n",
        "\n",
        "## ‚ö†Ô∏è Spark ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "- ‚ùå **‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Database** ‚Äî Spark ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏≤‡∏ß‡∏£\n",
        "- ‚ùå **‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Storage System** ‚Äî Spark ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å HDFS, S3, Parquet ‡∏Ø‡∏•‡∏Ø\n",
        "- ‚úÖ **Spark = Compute Layer** ‚Äî ‡πÄ‡∏õ‡πá‡∏ô \"‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•\" ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
        "\n",
        "### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Spark ‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô\n",
        "\n",
        "| ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠ | ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö | ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß |\n",
        "|-----------|---------|-----------|---------|\n",
        "| **Pandas** | ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, EDA | MB - ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô RAM | ‡πÄ‡∏£‡πá‡∏ß (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡πá‡∏Å) |\n",
        "| **DuckDB** | SQL Analytics ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß | MB - GB | ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å |\n",
        "| **Spark** | Distributed, Data Pipeline | GB - PB | ‡πÄ‡∏£‡πá‡∏ß (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏ç‡πà) |\n",
        "| **Hadoop MR** | Batch ‡∏ö‡∏ô HDFS | TB - PB | ‡∏ä‡πâ‡∏≤ (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô disk ‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_6",
      "metadata": {
        "id": "cell_6"
      },
      "source": [
        "# 3. üèóÔ∏è ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° Spark ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å (Spark Architecture Deep Dive)\n",
        "\n",
        "## ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å 3 ‡∏™‡πà‡∏ß‡∏ô\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         Cluster Manager             ‚îÇ\n",
        "‚îÇ    (‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£: YARN/K8s)     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "           ‚îÇ ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£ CPU/RAM\n",
        "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "    ‚ñº             ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Driver ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ   Executors (N ‡∏ï‡∏±‡∏ß) ‚îÇ\n",
        "‚îÇ (‡∏™‡∏°‡∏≠‡∏á) ‚îÇ   ‚îÇ   (‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô)         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### üß† Driver (‡∏™‡∏°‡∏≠‡∏á) ‚Äî ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà:\n",
        "1. **‡∏™‡∏£‡πâ‡∏≤‡∏á SparkSession** ‚Äî ‡πÄ‡∏õ‡∏¥‡∏î‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Spark\n",
        "2. **‡∏™‡∏£‡πâ‡∏≤‡∏á Logical Plan** ‚Äî ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô\n",
        "3. **Optimize** ‚Äî ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ú‡πà‡∏≤‡∏ô Catalyst Optimizer)\n",
        "4. **‡πÅ‡∏ö‡πà‡∏á‡∏á‡∏≤‡∏ô** ‚Äî ‡πÅ‡∏ö‡πà‡∏á Job ‚Üí Stages ‚Üí Tasks\n",
        "5. **‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•** ‚Äî ‡∏î‡∏π‡∏ß‡πà‡∏≤ Executor ‡∏ó‡∏≥‡πÄ‡∏™‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á\n",
        "\n",
        "### ‚öôÔ∏è Executor (‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô) ‚Äî ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà:\n",
        "1. **‡∏£‡∏±‡∏ö Task ‡∏°‡∏≤‡∏ó‡∏≥** ‚Äî ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÜ\n",
        "2. **‡πÄ‡∏Å‡πá‡∏ö Cache** ‚Äî ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô RAM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥\n",
        "3. **Shuffle** ‚Äî ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
        "4. **‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•** ‚Äî ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà Driver\n",
        "\n",
        "### üîÑ ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: Job ‚Üí Stage ‚Üí Task ‚Üí Partition\n",
        "\n",
        "| ‡∏£‡∏∞‡∏î‡∏±‡∏ö | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ | ‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å |\n",
        "|------|----------|--------|\n",
        "| **Job** | ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà 1 ‡∏ä‡∏¥‡πâ‡∏ô | ‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å Action (‡πÄ‡∏ä‡πà‡∏ô `.count()`) |\n",
        "| **Stage** | ‡∏ä‡πà‡∏ß‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Shuffle | ‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î Shuffle |\n",
        "| **Task** | ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î | 1 Task ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 1 Partition |\n",
        "| **Partition** | ‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Partition ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |\n",
        "\n",
        "> üí° **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:** Job = ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡πâ‡∏≤‡∏ô, Stage = ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (‡πÄ‡∏ó‡∏ê‡∏≤‡∏ô/‡∏Å‡πà‡∏≠‡∏ú‡∏ô‡∏±‡∏á/‡∏°‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≤), Task = ‡∏ä‡πà‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô, Partition = ‡∏ß‡∏±‡∏™‡∏î‡∏∏‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡πà‡∏á‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "added_md_89bd6e99",
      "metadata": {
        "id": "added_md_89bd6e99"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/driver_internal.png?raw=1\" width=\"800\" alt=\"Driver Internals Diagram\">\n",
        "  <br><i>‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á Driver: SparkSession, DAGScheduler, TaskScheduler</i>\n",
        "</div>\n",
        "\n",
        "#### üß† ‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å Driver Internals\n",
        "- **SparkSession**: ‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î (Entry Point)\n",
        "- **DAGScheduler**: ‡πÅ‡∏õ‡∏•‡∏á Logical Plan ‡πÄ‡∏õ‡πá‡∏ô Physical Plan ‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô Stages (Stage 1, Stage 2)\n",
        "- **TaskScheduler**: ‡∏£‡∏±‡∏ö Stage ‡∏°‡∏≤‡πÅ‡∏ï‡∏Å‡πÄ‡∏õ‡πá‡∏ô Task ‡∏¢‡πà‡∏≠‡∏¢‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏´‡πâ Executor ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà\n",
        "- **SchedulerBackend**: ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏±‡∏ö Cluster Manager ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ (CPU/RAM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "img_spark_chef_analogy",
      "metadata": {
        "id": "img_spark_chef_analogy"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/spark_chef_analogy.png?raw=1\" width=\"800\" alt=\"Spark Architecture Analogy: Driver (Chef) ‡∏™‡∏±‡πà‡∏á‡∏á‡∏≤‡∏ô Executors (Cooks)\">\n",
        "  <br><i>Spark Architecture Analogy: Driver (Chef) ‡∏™‡∏±‡πà‡∏á‡∏á‡∏≤‡∏ô Executors (Cooks)</i>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_7",
      "metadata": {
        "id": "cell_7"
      },
      "source": [
        "### 3.1 üß† Deep Dive: SparkSession vs SparkContext\n",
        "\n",
        "‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô‡∏≠‡∏≤‡∏à‡∏™‡∏á‡∏™‡∏±‡∏¢‡∏ß‡πà‡∏≤ `SparkSession` ‡∏Å‡∏±‡∏ö `SparkContext` ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?\n",
        "\n",
        "#### 1. SparkContext (`sc`) ‚Äî The Engine üîß\n",
        "- ‡πÄ‡∏õ‡πá‡∏ô **Entry Point** ‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á Spark (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô 1.x)\n",
        "- ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cluster Manager (YARN, K8s, Standalone)\n",
        "- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Executors, Memory, ‡πÅ‡∏•‡∏∞ Job Scheduling\n",
        "- **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:** ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô **\"‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏ô‡∏ï‡πå\"** ‡∏Ç‡∏≠‡∏á‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå ‚Äî ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Å‡∏•‡πÑ‡∏Å‡∏†‡∏≤‡∏¢‡πÉ‡∏ô\n",
        "\n",
        "#### 2. SparkSession (`spark`) ‚Äî The Dashboard üöó\n",
        "- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Spark 2.0+\n",
        "- ‡πÄ‡∏õ‡πá‡∏ô **Unified Entry Point** ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß:\n",
        "  - `SparkContext` (Core)\n",
        "  - `SQLContext` (DataFrames/SQL)\n",
        "  - `HiveContext` (Hive tables)\n",
        "  - `StreamingContext` (Streaming)\n",
        "- **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:** ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô **\"‡∏Ñ‡∏ô‡∏Ç‡∏±‡∏ö\"** ‡∏´‡∏£‡∏∑‡∏≠ **\"‡πÅ‡∏ú‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏±‡∏î\"** ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏£‡∏≤‡∏Ç‡∏±‡∏ö‡∏£‡∏ñ‡∏ú‡πà‡∏≤‡∏ô‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÑ‡∏õ‡∏´‡∏°‡∏∏‡∏ô‡∏•‡πâ‡∏≠‡πÄ‡∏≠‡∏á)\n",
        "\n",
        "> üí° **Best Practice:** ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô **‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ `SparkSession` ‡πÄ‡∏™‡∏°‡∏≠** (‡πÅ‡∏ï‡πà‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏°‡∏±‡∏ô‡∏Å‡πá‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ `SparkContext` ‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_8",
      "metadata": {
        "id": "cell_8"
      },
      "source": [
        "## üî¨ Lab: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Spark\n",
        "\n",
        "### Step 1: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á SparkSession\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_9",
      "metadata": {
        "id": "cell_9"
      },
      "outputs": [],
      "source": [
        "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PySpark (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Colab)\n",
        "!pip -q install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á SparkSession ‚Äî ‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Spark\n",
        "# üí° config(\"spark.ui.port\", \"4050\") ‡∏Å‡∏≥‡∏´‡∏ô‡∏î port ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á port 4040 ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ä‡∏ô‡∏Å‡∏±‡∏ô\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigData-Week4-DeepDive\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á SparkContext ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å wrap ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(f\"üì± App Name: {sc.appName}\")\n",
        "print(f\"üñ•Ô∏è Master: {sc.master}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_10",
      "metadata": {
        "id": "cell_10"
      },
      "source": [
        "### 3.2 üõ†Ô∏è ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: ‡πÄ‡∏õ‡∏¥‡∏î Spark UI ‡πÉ‡∏ô Google Colab\n",
        "\n",
        "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:** ‡∏õ‡∏Å‡∏ï‡∏¥ Spark UI ‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà `localhost:4040` ‡πÅ‡∏ï‡πà Google Colab ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á Server (Virtual Machine) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á `localhost` ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n",
        "\n",
        "**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:** ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ library `google.colab.output` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Proxy ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏î‡πâ\n",
        "\n",
        "‡∏£‡∏±‡∏ô cell ‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î Spark UI üëá\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_11",
      "metadata": {
        "id": "cell_11"
      },
      "outputs": [],
      "source": [
        "# üõ†Ô∏è Code ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î Spark UI ‡πÉ‡∏ô Colab\n",
        "try:\n",
        "    from google.colab import output\n",
        "\n",
        "    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î Spark UI\n",
        "    def show_spark_ui(port=4050):\n",
        "        # ‡∏î‡∏∂‡∏á URL ‡∏Ç‡∏≠‡∏á Spark UI ‡∏ú‡πà‡∏≤‡∏ô Proxy ‡∏Ç‡∏≠‡∏á Colab\n",
        "        url = output.serve_kernel_port_as_iframe(port)\n",
        "        # ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô Link ‡πÉ‡∏´‡πâ‡∏Ñ‡∏•‡∏¥‡∏Å (‡∏ñ‡πâ‡∏≤ iframe ‡πÄ‡∏•‡πá‡∏Å‡πÑ‡∏õ)\n",
        "        # output.serve_kernel_port_as_window(port)\n",
        "        print(f\"üöÄ Spark UI ‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà: {url}\")\n",
        "        print(\"üí° ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏Ñ‡∏•‡∏¥‡∏Å link ‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏ô (‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á login Google):\")\n",
        "        output.serve_kernel_port_as_window(port, path='/jobs/')\n",
        "\n",
        "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Default port ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡∏Ñ‡∏∑‡∏≠ 4050)\n",
        "    show_spark_ui(4050)\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ô‡∏ö‡∏ô Google Colab ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö library google.colab\")\n",
        "    print(f\"üëâ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô‡∏ö‡∏ô Local Jupyter ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏¥‡∏î: http://localhost:4050\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_12",
      "metadata": {
        "id": "cell_12"
      },
      "source": [
        "# 4. üè¢ Cluster Managers (‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£)\n",
        "\n",
        "Cluster Manager ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£ CPU ‡πÅ‡∏•‡∏∞ RAM ‡πÉ‡∏´‡πâ Spark ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö \"‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£\" ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏´‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡πà‡∏≤\n",
        "\n",
        "## ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Cluster Manager 3 ‡πÅ‡∏ö‡∏ö\n",
        "\n",
        "| | **Standalone** | **YARN** | **Kubernetes** |\n",
        "|---|---|---|---|\n",
        "| **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢** | Cluster Manager ‡∏Ç‡∏≠‡∏á Spark ‡πÄ‡∏≠‡∏á | Hadoop ecosystem | Container-based |\n",
        "| **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á** | ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î | ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Hadoop | ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ K8s cluster |\n",
        "| **‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö** | ‡∏ó‡∏î‡∏•‡∏≠‡∏á, ‡∏ó‡∏µ‡∏°‡πÄ‡∏•‡πá‡∏Å | ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡πÉ‡∏´‡∏ç‡πà | Cloud-native |\n",
        "| **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ** | Simple, ‡πÄ‡∏£‡πá‡∏ß | Queue management, ‡πÅ‡∏ä‡∏£‡πå resource | Auto-scaling, Isolate |\n",
        "| **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á** | Spark Standalone | AWS EMR, CDH | GKE, EKS |\n",
        "\n",
        "> üí° **‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏ô‡∏µ‡πâ** ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ `local[*]` ‡∏ã‡∏∂‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á \"‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å CPU core\" ‚Äî ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_13",
      "metadata": {
        "id": "cell_13"
      },
      "outputs": [],
      "source": [
        "# ‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ Cluster Manager ‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô\n",
        "print(f\"üè¢ Master URL: {spark.sparkContext.master}\")\n",
        "print()\n",
        "print(\"üìù ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Master URL:\")\n",
        "print(\"  local     = ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, 1 thread\")\n",
        "print(\"  local[*]  = ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å core\")\n",
        "print(\"  local[4]  = ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, 4 threads\")\n",
        "print(\"  yarn      = ‡πÉ‡∏ä‡πâ YARN ‡∏ö‡∏ô Hadoop cluster\")\n",
        "print(\"  k8s://... = ‡πÉ‡∏ä‡πâ Kubernetes cluster\")\n",
        "print(\"  spark://  = ‡πÉ‡∏ä‡πâ Standalone cluster\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_14",
      "metadata": {
        "id": "cell_14"
      },
      "source": [
        "# 5. ‚è≥ Lazy Evaluation (‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å)\n",
        "\n",
        "## ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Spark\n",
        "\n",
        "> **Spark ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ!** Spark ‡∏à‡∏∞ \"‡∏à‡∏î‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\" ‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£ ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏ó‡∏≥‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ\n",
        "\n",
        "### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
        "\n",
        "| | Pandas (Eager) | Spark (Lazy) |\n",
        "|---|---|---|\n",
        "| **‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á** | ‡∏ó‡∏≥‡∏ó‡∏±‡∏ô‡∏ó‡∏µ | ‡∏à‡∏î‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏£‡πâ‡∏≤‡∏á Plan) |\n",
        "| **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ** | ‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡πÄ‡∏•‡∏¢ | ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥ |\n",
        "| **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢** | ‡πÑ‡∏°‡πà optimize ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô | ‡∏ï‡πâ‡∏≠‡∏á trigger ‡∏î‡πâ‡∏ß‡∏¢ Action |\n",
        "\n",
        "### Transformation vs Action\n",
        "\n",
        "#### üîπ Transformations (Lazy ‚Äî ‡∏à‡∏î‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ó‡∏≥)\n",
        "| ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á | ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà |\n",
        "|--------|--------|\n",
        "| `select()` | ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå |\n",
        "| `filter()` / `where()` | ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß |\n",
        "| `groupBy()` | ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° |\n",
        "| `join()` | ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡∏≤‡∏£‡∏≤‡∏á |\n",
        "| `withColumn()` | ‡πÄ‡∏û‡∏¥‡πà‡∏°/‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå |\n",
        "| `orderBy()` | ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö |\n",
        "\n",
        "#### üî∏ Actions (‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô ‚Äî ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á!)\n",
        "| ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á | ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà |\n",
        "|--------|--------|\n",
        "| `show()` | ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• |\n",
        "| `count()` | ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß |\n",
        "| `collect()` | ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡∏ó‡∏µ‡πà Driver |\n",
        "| `write()` | ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á disk |\n",
        "| `take(n)` | ‡∏î‡∏∂‡∏á n ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "img_lazy_evaluation",
      "metadata": {
        "id": "img_lazy_evaluation"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/lazy_evaluation.png?raw=1\" width=\"800\" alt=\"Lazy Evaluation: ‡∏à‡∏î‡∏≠‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏≠‡∏£‡πå (Transformation) vs ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡∏≠‡∏≤‡∏´‡∏≤‡∏£ (Action)\">\n",
        "  <br><i>Lazy Evaluation: ‡∏à‡∏î‡∏≠‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏≠‡∏£‡πå (Transformation) vs ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡∏≠‡∏≤‡∏´‡∏≤‡∏£ (Action)</i>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_15",
      "metadata": {
        "id": "cell_15"
      },
      "source": [
        "### üî¨ Lab: ‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå Lazy Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_16",
      "metadata": {
        "id": "cell_16"
      },
      "outputs": [],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
        "data = [(i, f\"name_{i}\", i * 100, \"A\" if i % 2 == 0 else \"B\")\n",
        "        for i in range(1, 10001)]\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"salary\", \"dept\"])\n",
        "\n",
        "print(f\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏Ç‡∏ô‡∏≤‡∏î {df.count()} ‡πÅ‡∏ñ‡∏ß\")\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_17",
      "metadata": {
        "id": "cell_17"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# ===== Transformation (Lazy) ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô! =====\n",
        "print(\"üîπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Transformation...\")\n",
        "t0 = time.time()\n",
        "\n",
        "df2 = df.select(\"id\", \"salary\", \"dept\")      # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
        "df3 = df2.filter(df2[\"salary\"] > 500)         # ‡∏Å‡∏£‡∏≠‡∏á\n",
        "df4 = df3.groupBy(\"dept\").count()             # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°\n",
        "\n",
        "t1 = time.time()\n",
        "print(f\"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Transformation): {t1-t0:.4f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "print(\"üí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Spark ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢!\")\n",
        "\n",
        "# ===== Action (Trigger!) ‚Äî ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ Spark ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á! =====\n",
        "print(\"\\nüî∏ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Action (.show()) ‚Äî Spark ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô!\")\n",
        "t2 = time.time()\n",
        "\n",
        "df4.show()\n",
        "\n",
        "t3 = time.time()\n",
        "print(f\"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Action): {t3-t2:.4f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "print(\"üí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Spark ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥ Transformation ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_18",
      "metadata": {
        "id": "cell_18"
      },
      "source": [
        "### ‚úèÔ∏è ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 2: Transformation vs Action\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÑ‡∏´‡∏ô‡πÄ‡∏õ‡πá‡∏ô Transformation (T) ‡∏´‡∏£‡∏∑‡∏≠ Action (A)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_19",
      "metadata": {
        "id": "cell_19"
      },
      "outputs": [],
      "source": [
        "# ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏ï‡∏¥‡∏° T (Transformation) ‡∏´‡∏£‡∏∑‡∏≠ A (Action)\n",
        "\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 1: df.select(\"name\", \"salary\")      ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 2: df.filter(df[\"salary\"] > 1000)    ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 3: df.count()                        ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 4: df.groupBy(\"dept\").avg(\"salary\")  ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 5: df.show(10)                       ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 6: df.orderBy(\"salary\")              ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 7: df.collect()                      ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 8: df.withColumn(\"bonus\", df[\"salary\"] * 0.1)  ‚Üí  ________\n",
        "\n",
        "answers = {\n",
        "    1: \"________\",\n",
        "    2: \"________\",\n",
        "    3: \"________\",\n",
        "    4: \"________\",\n",
        "    5: \"________\",\n",
        "    6: \"________\",\n",
        "    7: \"________\",\n",
        "    8: \"________\",\n",
        "}\n",
        "\n",
        "for k, v in answers.items():\n",
        "    print(f\"‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_20",
      "metadata": {
        "id": "cell_20"
      },
      "source": [
        "# 6. üìä DAG (Directed Acyclic Graph)\n",
        "\n",
        "## DAG ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "DAG ‡∏Ñ‡∏∑‡∏≠ **‡∏Å‡∏£‡∏≤‡∏ü** ‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Spark ‡πÇ‡∏î‡∏¢:\n",
        "- **Directed** = ‡∏°‡∏µ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á (‡∏ó‡∏≥‡∏à‡∏≤‡∏Å‡∏ã‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏Ç‡∏ß‡∏≤)\n",
        "- **Acyclic** = ‡πÑ‡∏°‡πà‡∏ß‡∏ô‡∏ã‡πâ‡∏≥ (‡πÑ‡∏°‡πà‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥)\n",
        "- **Graph** = ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á\n",
        "\n",
        "```\n",
        "Read CSV ‚Üí Filter ‚Üí Select ‚Üí GroupBy ‚Üí [Shuffle] ‚Üí Aggregate ‚Üí Show\n",
        "  Stage 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  Stage 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂\n",
        "```\n",
        "\n",
        "## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô\n",
        "\n",
        "1. **Logical Plan** ‚Äî ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô\n",
        "2. **Optimized Logical Plan** ‚Äî Catalyst Optimizer ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\n",
        "3. **Physical Plan** ‚Äî ‡πÅ‡∏ú‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥ (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)\n",
        "\n",
        "> üí° ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô GPS ‚Äî ‡πÄ‡∏£‡∏≤‡∏ö‡∏≠‡∏Å‡∏à‡∏∏‡∏î‡∏´‡∏°‡∏≤‡∏¢ (Logical) ‚Üí GPS ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Optimized) ‚Üí ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á (Physical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_21",
      "metadata": {
        "id": "cell_21"
      },
      "outputs": [],
      "source": [
        "# ‡∏î‡∏π Execution Plan ‡∏Ç‡∏≠‡∏á Spark\n",
        "df_plan = df.select(\"id\", \"salary\", \"dept\") \\\n",
        "            .filter(df[\"salary\"] > 500) \\\n",
        "            .groupBy(\"dept\") \\\n",
        "            .avg(\"salary\")\n",
        "\n",
        "print(\"üìã Execution Plan:\")\n",
        "print(\"=\" * 50)\n",
        "df_plan.explain(\"formatted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_22",
      "metadata": {
        "id": "cell_22"
      },
      "outputs": [],
      "source": [
        "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: ‡∏î‡∏π‡πÅ‡∏ú‡∏ô‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (extended)\n",
        "print(\"üìã Extended Plan (‡∏î‡∏π Logical ‚Üí Physical):\")\n",
        "print(\"=\" * 50)\n",
        "df_plan.explain(\"extended\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_23",
      "metadata": {
        "id": "cell_23"
      },
      "source": [
        "# 7. üîÄ Shuffle ‚Äî ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á\n",
        "\n",
        "## Shuffle ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "Shuffle ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ **‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á** (‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏° partition) ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Spark ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà\n",
        "\n",
        "### ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡πÄ‡∏Å‡∏¥‡∏î Shuffle?\n",
        "\n",
        "| ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á | ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Shuffle | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á |\n",
        "|--------|-----------------|---------|\n",
        "| `groupBy()` | ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß | ‡∏ô‡∏±‡∏ö‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î |\n",
        "| `join()` | ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å 2 ‡∏ï‡∏≤‡∏£‡∏≤‡∏á | JOIN orders ‡∏Å‡∏±‡∏ö customers |\n",
        "| `orderBy()` | ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î | ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢ |\n",
        "| `distinct()` | ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î | ‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô user ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥ |\n",
        "\n",
        "### ‡∏ó‡∏≥‡πÑ‡∏° Shuffle ‡∏ñ‡∏∂‡∏á \"‡πÅ‡∏û‡∏á\"?\n",
        "\n",
        "> ‚ö†Ô∏è **Shuffle = ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢ = ‡∏ä‡πâ‡∏≤!**\n",
        "\n",
        "```\n",
        "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á 1: [A1, B2, A3]  ‚îÄ‚îÄ‚îÄ‚îÄ Network ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á 1: [A1, A3, A5] (‡∏Å‡∏•‡∏∏‡πà‡∏° A)\n",
        "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á 2: [B4, A5, B6]  ‚îÄ‚îÄ‚îÄ‚îÄ Network ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á 2: [B2, B4, B6] (‡∏Å‡∏•‡∏∏‡πà‡∏° B)\n",
        "```\n",
        "\n",
        "- ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô disk ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á (Spill)\n",
        "- ‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô Network (‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ RAM 1000 ‡πÄ‡∏ó‡πà‡∏≤)\n",
        "- ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "img_shuffle_concept",
      "metadata": {
        "id": "img_shuffle_concept"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/shuffle_concept.png?raw=1\" width=\"800\" alt=\"Shuffle Concept: ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏™‡∏µ‡∏•‡∏π‡∏Å‡∏ö‡∏≠‡∏•‡∏•‡∏á‡∏ñ‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\">\n",
        "  <br><i>Shuffle Concept: ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏™‡∏µ‡∏•‡∏π‡∏Å‡∏ö‡∏≠‡∏•‡∏•‡∏á‡∏ñ‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á</i>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_24",
      "metadata": {
        "id": "cell_24"
      },
      "outputs": [],
      "source": [
        "# ‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á: ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ Shuffle vs ‡πÑ‡∏°‡πà‡∏°‡∏µ Shuffle\n",
        "import time\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô\n",
        "big_data = [(i, f\"dept_{i % 50}\", i * 10.5) for i in range(100000)]\n",
        "big_df = spark.createDataFrame(big_data, [\"id\", \"department\", \"amount\"])\n",
        "\n",
        "# ===== ‡πÑ‡∏°‡πà‡∏°‡∏µ Shuffle (Narrow Transformation) =====\n",
        "t0 = time.time()\n",
        "result1 = big_df.filter(big_df[\"amount\"] > 5000).select(\"id\", \"amount\")\n",
        "result1.count()  # trigger action\n",
        "t1 = time.time()\n",
        "print(f\"üü¢ Filter + Select (‡πÑ‡∏°‡πà‡∏°‡∏µ Shuffle): {t1-t0:.3f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "\n",
        "# ===== ‡∏°‡∏µ Shuffle (Wide Transformation) =====\n",
        "t2 = time.time()\n",
        "result2 = big_df.groupBy(\"department\").sum(\"amount\")\n",
        "result2.count()  # trigger action\n",
        "t3 = time.time()\n",
        "print(f\"üî¥ GroupBy + Sum (‡∏°‡∏µ Shuffle): {t3-t2:.3f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "print(f\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: GroupBy ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ Filter ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á Shuffle!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_25",
      "metadata": {
        "id": "cell_25"
      },
      "source": [
        "# 8. üß™ Lab: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ Spark\n",
        "\n",
        "### Step 1: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_load_data",
      "metadata": {
        "scrolled": true,
        "id": "cell_load_data"
      },
      "outputs": [],
      "source": [
        "# 1. Download Austin Crime Data (Real World Dataset)\n",
        "# Dataset: Austin Crime Reports (from data.austintexas.gov)\n",
        "# File size: ~500 MB\n",
        "!wget -O crime.csv \"https://data.austintexas.gov/api/views/fdj4-gpfu/rows.csv?accessType=DOWNLOAD\"\n",
        "\n",
        "print(\"‚úÖ Downloaded crime.csv\")\n",
        "\n",
        "# 2. Load CSV into Spark DataFrame\n",
        "# inferSchema=True: ‡πÉ‡∏´‡πâ Spark ‡πÄ‡∏î‡∏≤ Type ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏≠‡∏≤‡∏à‡∏ä‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å)\n",
        "# header=True: ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
        "df_crime = spark.read.csv(\"crime.csv\", header=True, inferSchema=True)\n",
        "\n",
        "print(f\"‚úÖ Loaded Data: {df_crime.count():,} rows\")\n",
        "print(\"\\nüìä Original Schema:\")\n",
        "df_crime.printSchema()\n",
        "print(\"\\nüìã Sample Data:\")\n",
        "df_crime.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_common_mistakes_load",
      "metadata": {
        "id": "cell_common_mistakes_load"
      },
      "source": [
        "### ‚ö†Ô∏è Common Pitfalls: Data Loading\n",
        "\n",
        "| ‚ùå ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥ | üí• ‡∏ú‡∏•‡πÄ‡∏™‡∏µ‡∏¢ | ‚úÖ ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏ô |\n",
        "|---|---|---|\n",
        "| **inferSchema=True** ‡∏ö‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö TB | Spark ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå 1 ‡∏£‡∏≠‡∏ö ‚Üí ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å | ‡∏Å‡∏≥‡∏´‡∏ô‡∏î `schema=struct` ‡πÄ‡∏≠‡∏á |\n",
        "| ‡∏•‡∏∑‡∏° `header=True` | ‡πÄ‡∏≠‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å | `option(\"header\", \"true\")` |\n",
        "| ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏°‡∏µ Newline ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏±‡∏á/‡∏≠‡πà‡∏≤‡∏ô‡∏ú‡∏¥‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î | `option(\"multiLine\", \"true\")` |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_parquet_ex_md",
      "metadata": {
        "id": "cell_parquet_ex_md"
      },
      "source": [
        "### ‚ö° Exercise 0: CSV vs Parquet\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏õ‡πá‡∏ô Parquet ‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà\n",
        "\n",
        "**‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Parquet?**\n",
        "- ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ CSV ‡∏°‡∏≤‡∏Å (Column-oriented)\n",
        "- ‡πÄ‡∏Å‡πá‡∏ö Schema ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á guess Type)\n",
        "- ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ (Compression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_parquet_ex_code",
      "metadata": {
        "id": "cell_parquet_ex_code"
      },
      "outputs": [],
      "source": [
        "# TODO: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏õ‡∏•‡∏á CSV -> Parquet\n",
        "# 1. Save as Parquet (‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå crime_parquet)\n",
        "\n",
        "print(\"‚úÖ Saved to crime_parquet\")\n",
        "\n",
        "# 2. Read from Parquet\n",
        "\n",
        "# 3. Compare Count\n",
        "print(f\"‚úÖ Loaded Parquet: {df_crime_parquet.count():,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_step_2_trans",
      "metadata": {
        "id": "cell_step_2_trans"
      },
      "source": [
        "### Step 2: Transformation (Lazy)\n",
        "\n",
        "‡∏•‡∏≠‡∏á Filter ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πàClean) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤ Spark ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Lazy Evaluation ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_trans_demo",
      "metadata": {
        "id": "cell_trans_demo"
      },
      "outputs": [],
      "source": [
        "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏î‡∏µ 'THEFT' (‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå)\n",
        "# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô Austin Dataset ‡∏Ñ‡∏∑‡∏≠ 'Highest Offense Description'\n",
        "df_theft = df_crime_parquet.filter(df_crime_parquet[\"Highest Offense Description\"] == \"THEFT\")\n",
        "\n",
        "print(\"‚úÖ Transformation ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à (Spark ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_step_3_action",
      "metadata": {
        "id": "cell_step_3_action"
      },
      "source": [
        "### Step 3: Action (Trigger)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_action_demo",
      "metadata": {
        "id": "cell_action_demo"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "count = df_theft.count()\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏î‡∏µ‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå (THEFT): {count:,}\")\n",
        "print(f\"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {t1-t0:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_common_mistakes_action",
      "metadata": {
        "id": "cell_common_mistakes_action"
      },
      "source": [
        "### ‚ö†Ô∏è Common Pitfalls: Actions\n",
        "\n",
        "| ‚ùå ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á | ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ |\n",
        "|---|---|\n",
        "| **`collect()` ‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏ç‡πà** | Driver Memory ‡πÄ‡∏ï‡πá‡∏° (OOM) | ‡πÉ‡∏ä‡πâ `take(n)`, `show()`, ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á Disk |\n",
        "| **`count()` ‡∏ö‡πà‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ** | ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Spark ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏£‡∏≠‡∏ö (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà Cache) | `count()` ‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_step_4_explain",
      "metadata": {
        "id": "cell_step_4_explain"
      },
      "source": [
        "### Step 4: Execution Plan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_explain_demo",
      "metadata": {
        "id": "cell_explain_demo"
      },
      "outputs": [],
      "source": [
        "df_theft.explain(\"simple\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_deep_dive_clean",
      "metadata": {
        "id": "cell_deep_dive_clean"
      },
      "source": [
        "# 8. üßπ Deep Dive: Data Cleaning & Preparation\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÑ‡∏°‡πà Clean! ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ô\n",
        "\n",
        "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö:**\n",
        "1. ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡πÄ‡∏ä‡πà‡∏ô `Highest Offense Description`) ‚Üí Rename ‡πÄ‡∏õ‡πá‡∏ô `crime_type`\n",
        "2. `Occurred Date Time` ‡πÄ‡∏õ‡πá‡∏ô String (e.g. `01/01/2022 12:00:00 PM`) ‚Üí Convert to Timestamp\n",
        "3. ‡πÅ‡∏¢‡∏Å `Year`, `Month` ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_ex_clean_sql",
      "metadata": {
        "id": "cell_ex_clean_sql"
      },
      "source": [
        "### üõ†Ô∏è Exercise 1: Cleaning with Spark SQL\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÉ‡∏ä‡πâ Spark SQL ‡πÄ‡∏û‡∏∑‡πà‡∏≠:\n",
        "1. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
        "2. ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
        "3. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏µ 2020 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_clean_sql_code",
      "metadata": {
        "id": "cell_clean_sql_code"
      },
      "outputs": [],
      "source": [
        "# 1. Register Temp View\n",
        "df_crime_parquet.createOrReplaceTempView(\"crime_raw\")\n",
        "\n",
        "# 2. Write SQL\n",
        "df_clean_sql = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        `Incident Number` as case_id,\n",
        "        `Highest Offense Description` as crime_type,\n",
        "        coalesce(try_to_timestamp(`Occurred Date Time`, 'MM/dd/yyyy  HH:mm'), try_to_timestamp(`Occurred Date Time`, 'MM/dd/yyyy hh:mm:ss a')) as crime_date,\n",
        "        `Census Block Group` as census_block_group,\n",
        "        `Council District` as district,\n",
        "        `Clearance Status` as clearance_status\n",
        "    FROM crime_raw\n",
        "    WHERE `Occurred Date Time` IS NOT NULL\n",
        "\"\"\")\n",
        "\n",
        "df_clean_sql.printSchema()\n",
        "df_clean_sql.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0afca76",
      "metadata": {
        "id": "d0afca76"
      },
      "source": [
        "### üß© Exercise 1.2: SQL Challenge\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô SQL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ **Top 3 Districts** ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏î‡∏µ **BURGLARY** ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÉ‡∏ô‡∏õ‡∏µ **2023**\n",
        "\n",
        "‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (`____`) ‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44f566ab",
      "metadata": {
        "id": "44f566ab"
      },
      "outputs": [],
      "source": [
        "# TODO: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n",
        "sql_challenge = \"\"\"\n",
        "    SELECT\n",
        "        ____ as district,\n",
        "        COUNT(*) as crime_count\n",
        "    FROM crime_raw\n",
        "    WHERE crime_type = 'BURGLARY'\n",
        "      AND year = 2023\n",
        "    GROUP BY ____\n",
        "    ORDER BY ____ DESC\n",
        "    LIMIT 3\n",
        "\"\"\"\n",
        "\n",
        "# spark.sql(sql_challenge).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_ex_clean_py",
      "metadata": {
        "id": "cell_ex_clean_py"
      },
      "source": [
        "### üõ†Ô∏è Exercise 2: Cleaning with PySpark Functions\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏ó‡∏≥‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏ö‡∏ô ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ `pyspark.sql.functions` ‡πÅ‡∏ó‡∏ô SQL Strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_clean_py_code",
      "metadata": {
        "id": "cell_clean_py_code"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_clean = df_crime_parquet.select(\n",
        "    F.col(\"Incident Number\").alias(\"case_id\"),\n",
        "    F.col(\"Highest Offense Description\").alias(\"crime_type\"),\n",
        "    F.coalesce(F.to_timestamp(\"Occurred Date Time\", \"MM/dd/yyyy  HH:mm\"), F.to_timestamp(\"Occurred Date Time\", \"MM/dd/yyyy hh:mm:ss a\")).alias(\"crime_date\"),\n",
        "    F.col(\"Census Block Group\").cast(\"string\").alias(\"census_block_group\"),\n",
        "    F.col(\"Council District\").alias(\"district\"),\n",
        "    F.col(\"Clearance Status\").alias(\"clearance_status\"),\n",
        ") \\\n",
        ".withColumn(\"year\", F.year(\"crime_date\")) \\\n",
        ".withColumn(\"month\", F.month(\"crime_date\")) \\\n",
        ".filter(F.col(\"crime_date\").isNotNull())\n",
        "\n",
        "print(\"‚úÖ Cleaned Data Structure:\")\n",
        "df_clean.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606aa2d9",
      "metadata": {
        "id": "606aa2d9"
      },
      "source": [
        "### üß© Exercise 2.2: PySpark Transformations\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÉ‡∏ä‡πâ PySpark ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏î‡∏µ‡∏ó‡∏µ‡πà **Clearance Status** ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô NULL ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå `case_id`, `crime_type`, `district`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06bad55d",
      "metadata": {
        "id": "06bad55d"
      },
      "outputs": [],
      "source": [
        "# TODO: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n",
        "df_challenge = df_clean.select(\n",
        "    F.col(\"____\"),\n",
        "    F.col(\"____\"),\n",
        "    F.col(\"district\")\n",
        ").filter(\n",
        "    F.col(\"____\").isNotNull()\n",
        ")\n",
        "\n",
        "# df_challenge.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_common_mistakes_trans",
      "metadata": {
        "id": "cell_common_mistakes_trans"
      },
      "source": [
        "### ‚ö†Ô∏è Common Pitfalls: Transformation\n",
        "\n",
        "| ‚ùå ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á | ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ |\n",
        "|---|---|\n",
        "| **Chaining `withColumn`** ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ | Spark ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á plan ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏•‡∏∂‡∏Å‡∏°‡∏≤‡∏Å ‚Üí ‡∏ä‡πâ‡∏≤ | ‡πÉ‡∏ä‡πâ `select()` ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß |\n",
        "| ‡πÉ‡∏ä‡πâ **Python Function (UDF)** | ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ Python | ‡πÉ‡∏ä‡πâ **Spark Built-in Functions** (`F.xx`) ‡πÄ‡∏™‡∏°‡∏≠ |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_analysis_ex",
      "metadata": {
        "id": "cell_analysis_ex"
      },
      "source": [
        "# 9. üìä Analysis Exercises\n",
        "\n",
        "‡πÉ‡∏ä‡πâ `df_clean` ‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_ex_top_crimes",
      "metadata": {
        "id": "cell_ex_top_crimes"
      },
      "outputs": [],
      "source": [
        "# ‡πÇ‡∏à‡∏ó‡∏¢‡πå 1: 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏≠‡∏≤‡∏ä‡∏ç‡∏≤‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "df_clean.groupBy(\"crime_type\").count().orderBy(F.col(\"count\").desc()).show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_ex_year_trend",
      "metadata": {
        "id": "cell_ex_year_trend"
      },
      "outputs": [],
      "source": [
        "# ‡πÇ‡∏à‡∏ó‡∏¢‡πå 2: ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏≠‡∏≤‡∏ä‡∏ç‡∏≤‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏≤‡∏¢‡∏õ‡∏µ (Count by Year)\n",
        "df_clean.groupBy(\"year\").count().orderBy(\"year\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_ex_clearance",
      "metadata": {
        "id": "cell_ex_clearance"
      },
      "outputs": [],
      "source": [
        "# ‡πÇ‡∏à‡∏ó‡∏¢‡πå 3: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏Ñ‡∏î‡∏µ (Clearance Status)\n",
        "df_clean.groupBy(\"clearance_status\").count().orderBy(F.col(\"count\").desc()).show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f285d68d",
      "metadata": {
        "id": "f285d68d"
      },
      "source": [
        "### üß© Analysis Challenge (Fill-in-the-blank)\n",
        "\n",
        "‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484992c3",
      "metadata": {
        "id": "484992c3"
      },
      "outputs": [],
      "source": [
        "# 4. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á crime_type\n",
        "# df_clean.select(\"crime_type\").______().count()\n",
        "\n",
        "# 5. ‡∏Å‡∏≤‡∏£ Join ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å (Lookup) ‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£ Shuffle?\n",
        "# df_large.join(F.______(df_small), \"id\")\n",
        "\n",
        "# 6. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ DataFrame ‡∏ó‡∏µ‡πà Cache ‡πÑ‡∏ß‡πâ ‡∏Ñ‡∏ß‡∏£‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∑‡∏ô RAM?\n",
        "# df.______() # hint: ‡∏ï‡∏£‡∏á‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏±‡∏ö persist"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_cache_perf",
      "metadata": {
        "id": "cell_cache_perf"
      },
      "source": [
        "# 10. üöÄ Cache Performance Test\n",
        "\n",
        "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ Cache ‡∏Å‡∏±‡∏ö DataFrame ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Clean ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_cache_code",
      "metadata": {
        "id": "cell_cache_code"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Force Transformation\n",
        "df_heavy = df_clean.filter(F.col(\"year\") >= 2020).groupBy(\"district\").count()\n",
        "\n",
        "# 1. No Cache\n",
        "t0 = time.time()\n",
        "df_heavy.show(5) # Action 1\n",
        "df_heavy.count() # Action 2\n",
        "t1 = time.time()\n",
        "print(f\"üî¥ No Cache: {t1-t0:.2f} sec\")\n",
        "\n",
        "# 2. With Cache\n",
        "df_clean.cache()\n",
        "t2 = time.time()\n",
        "df_heavy.show(5) # Action 1 (First time builds cache)\n",
        "df_heavy.count() # Action 2 (Hits cache)\n",
        "t3 = time.time()\n",
        "print(f\"üü¢ Cache: {t3-t2:.2f} sec\")\n",
        "\n",
        "df_clean.unpersist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_common_mistakes_perf",
      "metadata": {
        "id": "cell_common_mistakes_perf"
      },
      "source": [
        "### ‚ö†Ô∏è Common Pitfalls: Performance\n",
        "\n",
        "| ‚ùå ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á | ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ |\n",
        "|---|---|\n",
        "| **‡∏•‡∏∑‡∏° `unpersist()`** | RAM ‡πÄ‡∏ï‡πá‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô | `unpersist()` ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ DataFrame ‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß |\n",
        "| **Cache ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô RAM** | Disk Spill (‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°) | ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Cache ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥‡∏ö‡πà‡∏≠‡∏¢‡πÜ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ |\n",
        "| **Shuffle ‡∏ö‡∏ô Key ‡∏ó‡∏µ‡πà‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏ß (Skew)** | ‡∏ö‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏¢‡∏á | ‡πÉ‡∏ä‡πâ Salt Key ‡∏´‡∏£‡∏∑‡∏≠ Broadcast Join (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_bonus_map",
      "metadata": {
        "id": "cell_bonus_map"
      },
      "source": [
        "# 11. üó∫Ô∏è Bonus: Visualizing Crime on Map\n",
        "\n",
        "**Goal:** Plot ‡∏à‡∏∏‡∏î‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏ö‡∏ô‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ `folium`\n",
        "\n",
        "**‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô:** ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ Plot ‡πÅ‡∏Ñ‡πà **500 ‡∏à‡∏∏‡∏î** ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Browser ‡∏Ñ‡πâ‡∏≤‡∏á\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_map_code",
      "metadata": {
        "id": "cell_map_code"
      },
      "outputs": [],
      "source": [
        "# 1. Download Austin Census Block Groups GeoJSON\n",
        "import requests\n",
        "import json\n",
        "import folium\n",
        "\n",
        "geojson_url = \"https://data.austintexas.gov/api/views/dwa9-qvcr/rows.geojson?accessType=DOWNLOAD\"\n",
        "\n",
        "print(\"‚è≥ Downloading GeoJSON...\")\n",
        "try:\n",
        "    resp = requests.get(geojson_url)\n",
        "    austin_geo = resp.json()\n",
        "    print(\"‚úÖ Downloaded GeoJSON\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Failed to download GeoJSON: {e}\")\n",
        "    austin_geo = None\n",
        "\n",
        "if austin_geo:\n",
        "    # 2. Prepare Data\n",
        "    df_heatmap = df_clean.filter(F.col(\"census_block_group\").isNotNull()) \\\n",
        "                         .groupBy(\"census_block_group\").count() \\\n",
        "                         .toPandas()\n",
        "\n",
        "    # DEBUG: Check values\n",
        "    print(\"üîç Data Sample:\", df_heatmap['census_block_group'].head(5).tolist())\n",
        "    print(\"üîç GeoJSON Sample:\", [f['properties']['geoid'] for f in austin_geo['features'][:5]])\n",
        "\n",
        "    df_heatmap['census_block_group'] = df_heatmap['census_block_group'].astype(str)\n",
        "\n",
        "    # 3. Create Map\n",
        "    austin_map = folium.Map(location=[30.2672, -97.7431], zoom_start=11)\n",
        "\n",
        "    folium.Choropleth(\n",
        "        geo_data=austin_geo,\n",
        "        name=\"Crime Density\",\n",
        "        data=df_heatmap,\n",
        "        columns=[\"census_block_group\", \"count\"],\n",
        "        key_on=\"feature.properties.geoid\",\n",
        "        fill_color=\"YlOrRd\",\n",
        "        fill_opacity=0.7,\n",
        "        line_opacity=0.2,\n",
        "        nan_fill_color=\"white\",\n",
        "        legend_name=\"Crime Count per Block Group\"\n",
        "    ).add_to(austin_map)\n",
        "\n",
        "    folium.LayerControl().add_to(austin_map)\n",
        "    austin_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e526d2e",
      "metadata": {
        "id": "3e526d2e"
      },
      "outputs": [],
      "source": [
        "folium.LayerControl().add_to(austin_map)\n",
        "austin_map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_45",
      "metadata": {
        "id": "cell_45"
      },
      "source": [
        "# 12. üìå ‡∏™‡∏£‡∏∏‡∏õ (Summary)\n",
        "\n",
        "## ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ\n",
        "\n",
        "| ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | ‡∏™‡∏£‡∏∏‡∏õ |\n",
        "|--------|------|\n",
        "| **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÑ‡∏°‡πà‡∏û‡∏≠** | ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• > RAM/CPU ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Distributed Computing |\n",
        "| **Spark ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£** | Distributed Compute Engine (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà DB, ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Storage) |\n",
        "| **Architecture** | Driver (‡∏™‡∏°‡∏≠‡∏á) + Executors (‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô) + Cluster Manager (‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£) |\n",
        "| **Lazy Evaluation** | Transformation ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô, Action ‡∏™‡∏±‡πà‡∏á‡∏ó‡∏≥ ‚Üí ‡∏ä‡πà‡∏ß‡∏¢ optimize |\n",
        "| **DAG** | ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‚Üí Catalyst Optimizer ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß |\n",
        "| **Shuffle** | ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‚Üí ‡∏ä‡πâ‡∏≤ ‡πÅ‡∏û‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á |\n",
        "| **Cache** | ‡πÄ‡∏Å‡πá‡∏ö DataFrame ‡πÉ‡∏ô RAM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥ ‚Üí ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤ |\n",
        "| **explain()** | ‡∏î‡∏π‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ Spark ‡∏à‡∏∞‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ |\n",
        "\n",
        "## üîÆ ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤: Data Pipeline\n",
        "\n",
        "> ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 5: ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Pipeline)  \n",
        "> ingest ‚Üí clean ‚Üí transform ‚Üí store ‚Üí analyze\n",
        "\n",
        "---\n",
        "\n",
        "## üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
        "\n",
        "- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)\n",
        "- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/)\n",
        "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_46",
      "metadata": {
        "id": "cell_46"
      },
      "outputs": [],
      "source": [
        "# ‡∏õ‡∏¥‡∏î Spark Session\n",
        "# spark.stop() # Comment ‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Lab Test ‡∏ï‡πà‡∏≠\n",
        "print(\"‚úÖ ‡∏à‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å! ‡πÑ‡∏õ‡∏•‡∏∏‡∏¢ Lab Test ‡∏Å‡∏±‡∏ô‡∏ï‡πà‡∏≠ üöÄ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd71b47",
      "metadata": {
        "id": "abd71b47"
      },
      "source": [
        "# 13. üß™ Lab Test: Olist E-Commerce Analytics (Real World Data)\n",
        "\n",
        "**Dataset:** Brazilian E-Commerce Public Dataset by Olist  \n",
        "**Source:** [Kaggle / GitHub](https://github.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist)  \n",
        "**Goal:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á 3 ‡∏ï‡∏≤‡∏£‡∏≤‡∏á (Orders, Items, Products)\n",
        "\n",
        "### üì• Step 0: Download Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af772ccc",
      "metadata": {
        "id": "af772ccc"
      },
      "outputs": [],
      "source": [
        "# Download Dataset from GitHub\n",
        "!wget -q -O olist_orders.csv https://raw.githubusercontent.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist/master/olist_orders_dataset.csv\n",
        "!wget -q -O olist_items.csv https://raw.githubusercontent.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist/master/olist_order_items_dataset.csv\n",
        "!wget -q -O olist_products.csv https://raw.githubusercontent.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist/master/olist_products_dataset.csv\n",
        "!wget -q -O olist_customers.csv https://raw.githubusercontent.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist/master/olist_customers_dataset.csv\n",
        "\n",
        "print(\"‚úÖ Download Cleaned!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d738d414",
      "metadata": {
        "id": "d738d414"
      },
      "source": [
        "### üõ†Ô∏è Task 1: Load & Clean Data\n",
        "\n",
        "1. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏±‡πâ‡∏á 4 ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Spark DataFrame\n",
        "2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Schema ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Type (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\n",
        "3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Temp View ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SQL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931a9d2c",
      "metadata": {
        "id": "931a9d2c"
      },
      "outputs": [],
      "source": [
        "# TODO: Load Data\n",
        "# df_orders = ...\n",
        "# df_items = ...\n",
        "# df_products = ...\n",
        "# df_customers = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5626d3ce",
      "metadata": {
        "id": "5626d3ce"
      },
      "source": [
        "### üîó Task 2: Join Data\n",
        "\n",
        "‡∏à‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á `df_master` ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ Join ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n",
        "1. `orders` JOIN `items` (ON order_id)\n",
        "2. JOIN `products` (ON product_id)\n",
        "3. JOIN `customers` (ON customer_id)\n",
        "\n",
        "> **Tip:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏´‡∏•‡∏±‡∏á Join ‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î‡∏•‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c611bcf",
      "metadata": {
        "id": "5c611bcf"
      },
      "outputs": [],
      "source": [
        "# TODO: Join Data\n",
        "# df_master = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d198e3d",
      "metadata": {
        "id": "2d198e3d"
      },
      "source": [
        "### üìä Task 3: Analytics\n",
        "\n",
        "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ SQL ‡∏´‡∏£‡∏∑‡∏≠ PySpark ‡∏Å‡πá‡πÑ‡∏î‡πâ):\n",
        "\n",
        "1. **Top Products:** ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏´‡∏°‡∏ß‡∏î‡πÑ‡∏´‡∏ô (`product_category_name`) ‡∏°‡∏µ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏ß‡∏° (price) ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å?\n",
        "2. **Top Cities:** ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÑ‡∏´‡∏ô (`customer_city`) ‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å?\n",
        "3. **(Optional) Monthly Sales:** ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏ß‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£? (‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a0682a",
      "metadata": {
        "id": "24a0682a"
      },
      "outputs": [],
      "source": [
        "# TODO: Analytics Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab4fefe",
      "metadata": {
        "id": "aab4fefe"
      },
      "outputs": [],
      "source": [
        "# ‡∏õ‡∏¥‡∏î Spark Session ‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥ Lab Test ‡πÄ‡∏™‡∏£‡πá‡∏à\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}